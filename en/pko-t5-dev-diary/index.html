<!DOCTYPE html>
<html lang="ko" class="no-js">
    <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    
    

    
    

    
    

    
    

    <title>pko-T5 Dev Case-study | PAUST TechBlog</title>
    <meta name="description" content="PAUST shares the experiences for making pretrained model T5 by large-scale korean corpus">
    
        <meta name="keywords" content="modeling, shaple">
    

    <!-- Social: Twitter -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="pko-T5 Dev Case-study | PAUST TechBlog">
    <meta name="twitter:description" content="PAUST shares the experiences for making pretrained model T5 by large-scale korean corpus">

    
        <meta property="twitter:image" content="https://s.abcnews.com/images/Technology/AP_black_hole_kab_150226_16x9_992.jpg">
    
    
    

    <!-- Social: Facebook / Open Graph -->
    <meta property="og:url" content="/en/pko-t5-dev-diary/">
    <meta property="og:title" content="pko-T5 Dev Case-study | PAUST TechBlog">
    <meta property="og:image" content="https://s.abcnews.com/images/Technology/AP_black_hole_kab_150226_16x9_992.jpg">
    <meta property="og:description" content="PAUST shares the experiences for making pretrained model T5 by large-scale korean corpus">
    <meta property="og:site_name" content="PAUST TechBlog">

    <!-- Favicon -->
    <link rel="shortcut icon" href="/en/favicon.ico" type="image/x-icon" />

    <!-- Android Lolipop Theme Color -->
    <meta name="theme-color" content="#141414">

    <!-- Fonts -->
    <link href="https://fonts.googleapis.com/css?family=Titillium+Web:300,400,700" rel="stylesheet">

    <link rel="stylesheet" href="/en/assets/css/styles.css">
    <link rel="canonical" href="/en/pko-t5-dev-diary/">
    <link rel="alternate" type="application/rss+xml" title="PAUST TechBlog" href="/en/feed.xml" />

    <!-- Include extra styles -->
    

    <!-- JavaScript enabled/disabled -->
    <script>
        document.querySelector('html').classList.remove('no-js');
    </script>
</head>

    <body class="has-push-menu">
        





        <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" display="none" version="1.1"><defs><symbol id="icon-menu" viewBox="0 0 1024 1024"><path class="path1" d="M128 213.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 725.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 469.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5z"/></symbol><symbol id="icon-search" viewBox="0 0 951 1024"><path class="path1" d="M658.286 475.429q0-105.714-75.143-180.857t-180.857-75.143-180.857 75.143-75.143 180.857 75.143 180.857 180.857 75.143 180.857-75.143 75.143-180.857zM950.857 950.857q0 29.714-21.714 51.429t-51.429 21.714q-30.857 0-51.429-21.714l-196-195.429q-102.286 70.857-228 70.857-81.714 0-156.286-31.714t-128.571-85.714-85.714-128.571-31.714-156.286 31.714-156.286 85.714-128.571 128.571-85.714 156.286-31.714 156.286 31.714 128.571 85.714 85.714 128.571 31.714 156.286q0 125.714-70.857 228l196 196q21.143 21.143 21.143 51.429z"/></symbol><symbol id="icon-close" viewBox="0 0 1000 1000"><path d="M969.8,870.3c27,27.7,27,71.8,0,99.1C955.7,983,937.9,990,920,990c-17.9,0-35.7-7-49.7-20.7L500,599L129.6,969.4C115.6,983,97.8,990,79.9,990s-35.7-7-49.7-20.7c-27-27.3-27-71.4,0-99.1L400.9,500L30.3,129.3c-27-27.3-27-71.4,0-99.1c27.3-27,71.8-27,99.4,0L500,400.9L870.4,30.2c27.7-27,71.8-27,99.4,0c27,27.7,27,71.8,0,99.1L599.1,500L969.8,870.3z"/></symbol><symbol id="icon-twitter" viewBox="0 0 951 1024"><path class="path1" d="M925.714 233.143q-38.286 56-92.571 95.429 0.571 8 0.571 24 0 74.286-21.714 148.286t-66 142-105.429 120.286-147.429 83.429-184.571 31.143q-154.857 0-283.429-82.857 20 2.286 44.571 2.286 128.571 0 229.143-78.857-60-1.143-107.429-36.857t-65.143-91.143q18.857 2.857 34.857 2.857 24.571 0 48.571-6.286-64-13.143-106-63.714t-42-117.429v-2.286q38.857 21.714 83.429 23.429-37.714-25.143-60-65.714t-22.286-88q0-50.286 25.143-93.143 69.143 85.143 168.286 136.286t212.286 56.857q-4.571-21.714-4.571-42.286 0-76.571 54-130.571t130.571-54q80 0 134.857 58.286 62.286-12 117.143-44.571-21.143 65.714-81.143 101.714 53.143-5.714 106.286-28.571z"/></symbol><symbol id="icon-facebook" viewBox="0 0 585 1024"><path class="path1" d="M548 6.857v150.857h-89.714q-49.143 0-66.286 20.571t-17.143 61.714v108h167.429l-22.286 169.143h-145.143v433.714h-174.857v-433.714h-145.714v-169.143h145.714v-124.571q0-106.286 59.429-164.857t158.286-58.571q84 0 130.286 6.857z"/></symbol><symbol id="icon-clock" viewBox="0 0 1000 1000"><path d="M500,10C229.8,10,10,229.8,10,500c0,270.2,219.8,490,490,490c270.2,0,490-219.8,490-490C990,229.8,770.2,10,500,10z M500,910.2c-226.2,0-410.2-184-410.2-410.2c0-226.2,184-410.2,410.2-410.2c226.2,0,410.2,184,410.2,410.2C910.2,726.1,726.2,910.2,500,910.2z M753.1,374c8.2,11.9,5.2,28.1-6.6,36.3L509.9,573.7c-4.4,3.1-9.6,4.6-14.8,4.6c-4.1,0-8.3-1-12.1-3c-8.6-4.5-14-13.4-14-23.1V202.5c0-14.4,11.7-26.1,26.1-26.1c14.4,0,26.1,11.7,26.1,26.1v300l195.6-135.1C728.7,359.2,744.9,362.1,753.1,374z"/></symbol><symbol id="icon-calendar" viewBox="0 0 1000 1000"><path d="M920,500v420H80V500H920 M990,430H10v490c0,38.7,31.3,70,70,70h840c38.7,0,70-31.3,70-70V430L990,430z"/><path d="M850,80v105c0,57.9-47.2,105-105,105c-58,0-105-47.1-105-105V80H360v105c0,57.9-47.2,105-105,105c-58,0-105-47.1-105-105V80C72.8,80,10,142.7,10,220v140h980V220C990,142.7,927.2,80,850,80z"/><path d="M255,10c-19.3,0-35,15.8-35,35v140c0,19.2,15.7,35,35,35c19.3,0,35-15.8,35-35V45C290,25.8,274.3,10,255,10z"/><path d="M745,10c-19.3,0-35,15.8-35,35v140c0,19.2,15.7,35,35,35c19.3,0,35-15.8,35-35V45C780,25.8,764.3,10,745,10z"/></symbol><symbol id="icon-github" viewBox="0 0 12 14"><path d="M6 1q1.633 0 3.012 0.805t2.184 2.184 0.805 3.012q0 1.961-1.145 3.527t-2.957 2.168q-0.211 0.039-0.312-0.055t-0.102-0.234q0-0.023 0.004-0.598t0.004-1.051q0-0.758-0.406-1.109 0.445-0.047 0.801-0.141t0.734-0.305 0.633-0.52 0.414-0.82 0.16-1.176q0-0.93-0.617-1.609 0.289-0.711-0.062-1.594-0.219-0.070-0.633 0.086t-0.719 0.344l-0.297 0.187q-0.727-0.203-1.5-0.203t-1.5 0.203q-0.125-0.086-0.332-0.211t-0.652-0.301-0.664-0.105q-0.352 0.883-0.062 1.594-0.617 0.68-0.617 1.609 0 0.664 0.16 1.172t0.41 0.82 0.629 0.523 0.734 0.305 0.801 0.141q-0.305 0.281-0.383 0.805-0.164 0.078-0.352 0.117t-0.445 0.039-0.512-0.168-0.434-0.488q-0.148-0.25-0.379-0.406t-0.387-0.187l-0.156-0.023q-0.164 0-0.227 0.035t-0.039 0.090 0.070 0.109 0.102 0.094l0.055 0.039q0.172 0.078 0.34 0.297t0.246 0.398l0.078 0.18q0.102 0.297 0.344 0.48t0.523 0.234 0.543 0.055 0.434-0.027l0.18-0.031q0 0.297 0.004 0.691t0.004 0.426q0 0.141-0.102 0.234t-0.312 0.055q-1.812-0.602-2.957-2.168t-1.145-3.527q0-1.633 0.805-3.012t2.184-2.184 3.012-0.805zM2.273 9.617q0.023-0.055-0.055-0.094-0.078-0.023-0.102 0.016-0.023 0.055 0.055 0.094 0.070 0.047 0.102-0.016zM2.516 9.883q0.055-0.039-0.016-0.125-0.078-0.070-0.125-0.023-0.055 0.039 0.016 0.125 0.078 0.078 0.125 0.023zM2.75 10.234q0.070-0.055 0-0.148-0.062-0.102-0.133-0.047-0.070 0.039 0 0.141t0.133 0.055zM3.078 10.562q0.062-0.062-0.031-0.148-0.094-0.094-0.156-0.023-0.070 0.062 0.031 0.148 0.094 0.094 0.156 0.023zM3.523 10.758q0.023-0.086-0.102-0.125-0.117-0.031-0.148 0.055t0.102 0.117q0.117 0.047 0.148-0.047zM4.016 10.797q0-0.102-0.133-0.086-0.125 0-0.125 0.086 0 0.102 0.133 0.086 0.125 0 0.125-0.086zM4.469 10.719q-0.016-0.086-0.141-0.070-0.125 0.023-0.109 0.117t0.141 0.062 0.109-0.109z"></path></symbol><symbol id="icon-medium" viewBox="0 0 1000 1000"><path d="M336.5,240.2v641.5c0,9.1-2.3,16.9-6.8,23.2s-11.2,9.6-20,9.6c-6.2,0-12.2-1.5-18-4.4L37.3,782.7c-7.7-3.6-14.1-9.8-19.4-18.3S10,747.4,10,739V115.5c0-7.3,1.8-13.5,5.5-18.6c3.6-5.1,8.9-7.7,15.9-7.7c5.1,0,13.1,2.7,24.1,8.2l279.5,140C335.9,238.6,336.5,239.5,336.5,240.2L336.5,240.2z M371.5,295.5l292,473.6l-292-145.5V295.5z M990,305.3v576.4c0,9.1-2.6,16.5-7.7,22.1c-5.1,5.7-12,8.5-20.8,8.5s-17.3-2.4-25.7-7.1L694.7,784.9L990,305.3z M988.4,239.7c0,1.1-46.8,77.6-140.3,229.4C754.6,621,699.8,709.8,683.8,735.7L470.5,389l177.2-288.2c6.2-10.2,15.7-15.3,28.4-15.3c5.1,0,9.8,1.1,14.2,3.3l295.9,147.7C987.6,237.1,988.4,238.2,988.4,239.7L988.4,239.7z"/></symbol><symbol id="icon-instagram" viewBox="0 0 489.84 489.84"><path d="M249.62,50.46c65.4,0,73.14.25,99,1.43C372.47,53,385.44,57,394.07,60.32a75.88,75.88,0,0,1,28.16,18.32,75.88,75.88,0,0,1,18.32,28.16c3.35,8.63,7.34,21.6,8.43,45.48,1.18,25.83,1.43,33.57,1.43,99s-0.25,73.14-1.43,99c-1.09,23.88-5.08,36.85-8.43,45.48a81.11,81.11,0,0,1-46.48,46.48c-8.63,3.35-21.6,7.34-45.48,8.43-25.82,1.18-33.57,1.43-99,1.43s-73.15-.25-99-1.43c-23.88-1.09-36.85-5.08-45.48-8.43A75.88,75.88,0,0,1,77,423.86,75.88,75.88,0,0,1,58.69,395.7c-3.35-8.63-7.34-21.6-8.43-45.48-1.18-25.83-1.43-33.57-1.43-99s0.25-73.14,1.43-99c1.09-23.88,5.08-36.85,8.43-45.48A75.88,75.88,0,0,1,77,78.64a75.88,75.88,0,0,1,28.16-18.32c8.63-3.35,21.6-7.34,45.48-8.43,25.83-1.18,33.57-1.43,99-1.43m0-44.13c-66.52,0-74.86.28-101,1.47s-43.87,5.33-59.45,11.38A120.06,120.06,0,0,0,45.81,47.44,120.06,120.06,0,0,0,17.56,90.82C11.5,106.4,7.36,124.2,6.17,150.27s-1.47,34.46-1.47,101,0.28,74.86,1.47,101,5.33,43.87,11.38,59.45a120.06,120.06,0,0,0,28.25,43.38,120.06,120.06,0,0,0,43.38,28.25c15.58,6.05,33.38,10.19,59.45,11.38s34.46,1.47,101,1.47,74.86-.28,101-1.47,43.87-5.33,59.45-11.38a125.24,125.24,0,0,0,71.63-71.63c6.05-15.58,10.19-33.38,11.38-59.45s1.47-34.46,1.47-101-0.28-74.86-1.47-101-5.33-43.87-11.38-59.45a120.06,120.06,0,0,0-28.25-43.38,120.06,120.06,0,0,0-43.38-28.25C394.47,13.13,376.67,9,350.6,7.8s-34.46-1.47-101-1.47h0Z" transform="translate(-4.7 -6.33)" /><path d="M249.62,125.48A125.77,125.77,0,1,0,375.39,251.25,125.77,125.77,0,0,0,249.62,125.48Zm0,207.41a81.64,81.64,0,1,1,81.64-81.64A81.64,81.64,0,0,1,249.62,332.89Z" transform="translate(-4.7 -6.33)"/><circle cx="375.66" cy="114.18" r="29.39" /></symbol><symbol id="icon-linkedin" viewBox="0 0 12 14"><path d="M2.727 4.883v7.742h-2.578v-7.742h2.578zM2.891 2.492q0.008 0.57-0.395 0.953t-1.059 0.383h-0.016q-0.641 0-1.031-0.383t-0.391-0.953q0-0.578 0.402-0.957t1.051-0.379 1.039 0.379 0.398 0.957zM12 8.187v4.437h-2.57v-4.141q0-0.82-0.316-1.285t-0.988-0.465q-0.492 0-0.824 0.27t-0.496 0.668q-0.086 0.234-0.086 0.633v4.32h-2.57q0.016-3.117 0.016-5.055t-0.008-2.313l-0.008-0.375h2.57v1.125h-0.016q0.156-0.25 0.32-0.438t0.441-0.406 0.68-0.34 0.895-0.121q1.336 0 2.148 0.887t0.813 2.598z"></path></symbol><symbol id="icon-heart" viewBox="0 0 34 30"><path d="M17,29.7 L16.4,29.2 C3.5,18.7 0,15 0,9 C0,4 4,0 9,0 C13.1,0 15.4,2.3 17,4.1 C18.6,2.3 20.9,0 25,0 C30,0 34,4 34,9 C34,15 30.5,18.7 17.6,29.2 L17,29.7 Z M9,2 C5.1,2 2,5.1 2,9 C2,14.1 5.2,17.5 17,27.1 C28.8,17.5 32,14.1 32,9 C32,5.1 28.9,2 25,2 C21.5,2 19.6,4.1 18.1,5.8 L17,7.1 L15.9,5.8 C14.4,4.1 12.5,2 9,2 Z" id="Shape"></path></symbol><symbol id="icon-arrow-right" viewBox="0 0 25.452 25.452"><path d="M4.471,24.929v-2.004l12.409-9.788c0.122-0.101,0.195-0.251,0.195-0.411c0-0.156-0.073-0.31-0.195-0.409L4.471,2.526V0.522c0-0.2,0.115-0.384,0.293-0.469c0.18-0.087,0.396-0.066,0.552,0.061l15.47,12.202c0.123,0.1,0.195,0.253,0.195,0.409c0,0.16-0.072,0.311-0.195,0.411L5.316,25.34c-0.155,0.125-0.372,0.147-0.552,0.061C4.586,25.315,4.471,25.13,4.471,24.929z"/></symbol><symbol id="icon-star" viewBox="0 0 48 48"><path fill="currentColor" d="M44,24c0,11.045-8.955,20-20,20S4,35.045,4,24S12.955,4,24,4S44,12.955,44,24z"/><path fill="#ffffff" d="M24,11l3.898,7.898l8.703,1.301l-6.301,6.102l1.5,8.699L24,30.898L16.199,35l1.5-8.699l-6.301-6.102  l8.703-1.301L24,11z"/></symbol><symbol id="icon-read" viewBox="0 0 32 32"><path fill="currentColor" d="M29,4H3C1.343,4,0,5.343,0,7v18c0,1.657,1.343,3,3,3h10c0,0.552,0.448,1,1,1h4c0.552,0,1-0.448,1-1h10  c1.657,0,3-1.343,3-3V7C32,5.343,30.657,4,29,4z M29,5v20H18.708c-0.618,0-1.236,0.146-1.789,0.422l-0.419,0.21V5H29z M15.5,5  v20.632l-0.419-0.21C14.528,25.146,13.91,25,13.292,25H3V5H15.5z M31,25c0,1.103-0.897,2-2,2H18v1h-4v-1H3c-1.103,0-2-0.897-2-2V7  c0-0.737,0.405-1.375,1-1.722V25c0,0.552,0.448,1,1,1h10.292c0.466,0,0.925,0.108,1.342,0.317l0.919,0.46  c0.141,0.07,0.294,0.106,0.447,0.106c0.153,0,0.306-0.035,0.447-0.106l0.919-0.46C17.783,26.108,18.242,26,18.708,26H29  c0.552,0,1-0.448,1-1V5.278C30.595,5.625,31,6.263,31,7V25z M6,12.5C6,12.224,6.224,12,6.5,12h5c0.276,0,0.5,0.224,0.5,0.5  S11.776,13,11.5,13h-5C6.224,13,6,12.776,6,12.5z M6,14.5C6,14.224,6.224,14,6.5,14h5c0.276,0,0.5,0.224,0.5,0.5S11.776,15,11.5,15  h-5C6.224,15,6,14.776,6,14.5z M6,16.5C6,16.224,6.224,16,6.5,16h5c0.276,0,0.5,0.224,0.5,0.5S11.776,17,11.5,17h-5  C6.224,17,6,16.776,6,16.5z M20,12.5c0-0.276,0.224-0.5,0.5-0.5h5c0.276,0,0.5,0.224,0.5,0.5S25.776,13,25.5,13h-5  C20.224,13,20,12.776,20,12.5z M20,14.5c0-0.276,0.224-0.5,0.5-0.5h5c0.276,0,0.5,0.224,0.5,0.5S25.776,15,25.5,15h-5  C20.224,15,20,14.776,20,14.5z M20,16.5c0-0.276,0.224-0.5,0.5-0.5h5c0.276,0,0.5,0.224,0.5,0.5S25.776,17,25.5,17h-5  C20.224,17,20,16.776,20,16.5z"></path></symbol></defs></svg>

        <header class="bar-header">
    <a id="menu" role="button">
        <svg id="open" class="icon-menu"><use xlink:href="#icon-menu"></use></svg>
    </a>
    <h1 class="logo">
        <a href="/en/">
            
                PAUST TechBlog <span class="version"></span>
            
        </a>
    </h1>
    <a id="search" class="dosearch" role="button">
        <svg class="icon-search"><use xlink:href="#icon-search"></use></svg>
    </a>
    
</header>

<div id="mask" class="overlay"></div>

<aside class="sidebar" id="sidebar">
    <nav id="navigation">
      <h2>Menu</h2>
      <ul>
  
    
      <li>
        <a href="/en/">Home</a>
      </li>
    
  
</ul>

    </nav>
</aside>

<div class="search-wrapper">
    <div class="search-form">
        <input type="text" class="search-field" placeholder="Search">
        <svg class="icon-remove-sign"><use xlink:href="#icon-close"></use></svg>
        <ul class="search-results search-list"></ul>
    </div>
</div>



        <section class="post one-column">
            <article role="article" class="post-content">
                <p class="post-info">
                    <span style="float: right;">
                        <a href="/pko-t5-dev-diary/" style="color: grey;">한국어</a>
                        |
                        <a href="/en/pko-t5-dev-diary/" style="color: grey;">ENGLISH</a>
                    </span>

                    
                        <svg class="icon-calendar" id="date"><use xlink:href="#icon-calendar"></use></svg>
                        <time class="date" datetime="2022-08-24T20:21:00+09:00">
                            


August 24, 2022

                        </time>
                    
                    <svg id="clock" class="icon-clock"><use xlink:href="#icon-clock"></use></svg>
                    <span>10 min to read</span>
                </p>
                <h1 class="post-title">pko-T5 Dev Case-study</h1>
                <p class="post-subtitle">How PAUST developed T5 utilizing large-scale Korean corpus</p>

                
                    <img src="https://s.abcnews.com/images/Technology/AP_black_hole_kab_150226_16x9_992.jpg" alt="Featured image" class="post-cover">
                

                <!-- Pagination links -->



                <!-- Add your table of contents here -->


                <h1 id="introduction">Introduction</h1>
<p>PAUST has successfully developed Open-domain QA based on Korean with T5.</p>

<p>People have tried to utilize Facebook DPR and FiD to develop Open-domain QA.</p>

<p>There were Korean-based pre-training models for DPR, such as <a href="https://huggingface.co/monologg/kobert">monologg/koBERT</a> and <a href="https://huggingface.co/klue/roberta-base">klue/roberta-base</a>. However, for <a href="https://github.com/facebookresearch/FiD">FiD</a>, we need an encoder-decoder model rather than an encoder-only model.</p>

<p>T5 and Bart are representing encoder-decoder models. We needed 2 conditions in the below when experimenting with utilizing the encoder-decoder model based on Korean.</p>

<ol>
  <li>Make it available to receive special characters by making tokenizer based on BBPE.</li>
  <li>Make it based on <a href="https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511">T5 v1.1</a> model.</li>
</ol>

<p>In order to satisfy the above conditions, we tried T5 model training utilizing the Korean text corpus and transformers library.</p>

<p>We easily made BBPE thanks to tokenizers library. For T5 training which was originally realized with tensorflow, we made one utilizing PyTorch model of transformers.</p>

<p>First of all, as you can see from Fig. 1, T5 integrates the format of all datasets into text-to-text format. Therefore, it is very useful for it is possible to implement it in diverse NLP tasks.</p>

<p><img src="https://1.bp.blogspot.com/-o4oiOExxq1s/Xk26XPC3haI/AAAAAAAAFU8/NBlvOWB84L0PTYy9TzZBaLf6fwPGJTR0QCLcBGAsYHQ/s1600/image3.gif" alt="Fig.1 Text-to-text By Original Google T5" />
Fig.1 T5 integrating format into Text-to-text</p>

<p>This makes it available to be implemented for diverse NLP tasks such as Translation, Text classification, QA.</p>

<p>Creating a pre-training from scratch like this demands huge GPU resources. However, we needed to train diverse tasks with limited GPU resources. Therefore, it was very beneficial for us that it was okay to do some fine-tuning based on pre-trained knowledge for T5 model.</p>

<p>Please refer to the <a href="https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511">link</a> for the differences between T5 v1.1 and original T5.</p>

<h1 id="implementation">Implementation</h1>

<p>Fortunately, we had useful Korean data such as “모두의말뭉치” by the National Institute of Korean Language, Namu Wiki, and Wikipedia. At least for T5 v1.1, it only needed unsupervised learning, which meant we could exclude multitask-learning in the original study which made things more convenient.</p>

<p>Although the beginning of actual training was from <a href="https://github.com/huggingface/transformers/blob/main/examples/flax/language-modeling/run_t5_mlm_flax.py">run_mlm_t5_flax.py</a> which was provided as an example by <a href="https://github.com/huggingface/transformers">huggingface/transformers</a>, However, what Huggingface provides only decides the static ratio of T5 span to be created statically. Therefore, we couldn’t solely depend on Huggingface. So we did random active masking and then, merging of masked tokens. The code is below:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">_fill_in_the_blank</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">words</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]):</span>
    <span class="s">"""
    input:
    {
      words: [My, name, is, T5, .]
    }
    output:
    {
      inputs: [&lt;extra_id_1&gt;, name, is, &lt;extra_id_2&gt;, .]
      outputs: [My, &lt;extra_id_1&gt;, T5, .]
    }
    """</span>
    <span class="n">mask_id</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>

    <span class="n">min_prob</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">max_prob</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">2</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">copy</span><span class="p">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
    <span class="n">targets</span> <span class="o">=</span> <span class="n">words</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)):</span>
        <span class="n">prob</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="n">random</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">min_prob</span> <span class="o">&lt;</span> <span class="n">prob</span> <span class="o">&lt;</span> <span class="n">max_prob</span><span class="p">:</span>
            <span class="n">inputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">mask_id</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">targets</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">mask_id</span>

    <span class="k">def</span> <span class="nf">merge_mask</span><span class="p">(</span><span class="n">words_</span><span class="p">):</span>
        <span class="n">mask_spans</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">begin</span><span class="p">,</span> <span class="n">end</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">words_</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">w</span> <span class="o">==</span> <span class="n">mask_id</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">begin</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
                    <span class="n">begin</span> <span class="o">=</span> <span class="n">i</span>
                <span class="n">end</span> <span class="o">=</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">end</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                    <span class="n">mask_spans</span><span class="p">.</span><span class="n">append</span><span class="p">((</span><span class="n">begin</span><span class="p">,</span> <span class="n">end</span><span class="p">))</span>
                    <span class="n">begin</span><span class="p">,</span> <span class="n">end</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span>
        <span class="k">if</span> <span class="n">begin</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="ow">and</span> <span class="n">end</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">mask_spans</span><span class="p">.</span><span class="n">append</span><span class="p">((</span><span class="n">begin</span><span class="p">,</span> <span class="n">end</span><span class="p">))</span>

        <span class="n">new_words_</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">last_offset</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">mask_spans</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">extra_ids</span><span class="p">),</span> <span class="sa">f</span><span class="s">"mask_spans=</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">mask_spans</span><span class="p">)</span><span class="si">}</span><span class="s"> is over length of extra_ids"</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">begin</span><span class="p">,</span> <span class="n">end</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">mask_spans</span><span class="p">):</span>
            <span class="n">new_words_</span> <span class="o">+=</span> <span class="n">words_</span><span class="p">[</span><span class="n">last_offset</span><span class="p">:</span><span class="n">begin</span><span class="p">]</span>
            <span class="n">new_words_</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">extra_ids</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">last_offset</span> <span class="o">=</span> <span class="n">end</span>
        <span class="n">new_words_</span> <span class="o">+=</span> <span class="n">words_</span><span class="p">[</span><span class="n">last_offset</span><span class="p">:]</span>

        <span class="k">return</span> <span class="n">new_words_</span>

    <span class="n">inputs</span> <span class="o">=</span> <span class="n">merge_mask</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">targets</span> <span class="o">=</span> <span class="n">merge_mask</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span><span class="s">'inputs'</span><span class="p">:</span> <span class="n">inputs</span><span class="p">,</span> <span class="s">'targets'</span><span class="p">:</span> <span class="n">targets</span><span class="p">}</span>
</code></pre></div></div>

<p>We can prevent all targets to be masked from the code above if we set at least 1 token to be masked but limit the total percentage of tokens to be masked up to 50%.</p>

<p>T5 calls this task a “Span Corruption Task” and it works like the below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">SpanCorruptionTask</span><span class="p">(</span><span class="n">tokens</span><span class="o">=</span><span class="p">[</span><span class="s">'My'</span><span class="p">,</span> <span class="s">'name'</span><span class="p">,</span> <span class="s">'is'</span><span class="p">,</span> <span class="s">'T5'</span><span class="p">,</span> <span class="s">'.'</span><span class="p">])</span> <span class="o">=&gt;</span> <span class="p">{</span>
  <span class="n">inputs</span><span class="p">:</span> <span class="p">[</span><span class="s">'&lt;extra_id_1&gt;'</span><span class="p">,</span> <span class="s">'name'</span><span class="p">,</span> <span class="s">'is'</span><span class="p">,</span> <span class="s">'&lt;extra_id_2&gt;'</span><span class="p">,</span> <span class="s">'.'</span><span class="p">]</span>
  <span class="n">outputs</span><span class="p">:</span> <span class="p">[</span><span class="s">'My'</span><span class="p">,</span> <span class="s">'&lt;extra_id_1&gt;'</span><span class="p">,</span> <span class="s">'T5'</span><span class="p">,</span> <span class="s">'.'</span><span class="p">]</span>
<span class="p">}</span>
</code></pre></div></div>

<p>We utilized 8 pieces of A100 for pre-training, and it took 3 days, 16 days, and 26 days for small, base, and large respectively. We trained from 1 node of TPU v3-8 for fine-tuning of each task.</p>

<p>Detailed code for pre-training can be found here: <a href="https://github.com/paust-team/pko-t5">https://github.com/paust-team/pko-t5</a></p>

<p>We named this T5 model to pko-t5, short for PAUST Korea T5. We did diverse experiments utilizing this training model later. Details can be found in the next section.</p>

<h1 id="experiments">Experiments</h1>

<h2 id="nsmc">NSMC</h2>

<p>First, we transformed NSMC data into text-to-text format and experimented. Before processing the experiment, we utilized pko-t5 and gained the outcome below:</p>

<table>
  <thead>
    <tr>
      <th>Model name</th>
      <th>NSMC’s Accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>KoBART-base*</td>
      <td>90.24</td>
    </tr>
    <tr>
      <td>KETI-AIR/ke-t5-small-ko</td>
      <td>89.37</td>
    </tr>
    <tr>
      <td>KETI-AIR/ke-t5-base-ko</td>
      <td><strong>91.38</strong></td>
    </tr>
    <tr>
      <td>KETI-AIR/ke-t5-large-ko</td>
      <td>90.67</td>
    </tr>
    <tr>
      <td>paust/pko-t5-small</td>
      <td>89.54</td>
    </tr>
    <tr>
      <td>paust/pko-t5-base</td>
      <td>91.05</td>
    </tr>
    <tr>
      <td>paust/pko-t5-large</td>
      <td>91.15</td>
    </tr>
    <tr>
      <td>google/mt5-base</td>
      <td>87.63</td>
    </tr>
  </tbody>
</table>

<p>*: Please refer to <a href="https://github.com/SKT-AI/KoBART">KoBART-base</a> announced by SKT.</p>

<p>Although we had less outcome than ke-t5, the performance was within the error range. We designed the document → label generation for training with NSMC data for the text-to-text performance.</p>

<p>For example, BERT style and text-to-text style proceeds the sentence below differently:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Input</span><span class="p">:</span> <span class="s">"pko-t5 프로젝트는 정말 멋지고 훌륭하네요."</span>
<span class="n">BERT</span> <span class="n">label</span><span class="p">:</span> <span class="mi">1</span> <span class="p">(</span><span class="n">positive</span> <span class="n">label</span><span class="p">)</span>
<span class="n">T5</span> <span class="n">label</span><span class="p">:</span> <span class="s">"긍정적인 댓글"</span>
</code></pre></div></div>

<p>For encoder-only architecture like BERT, we can use the weights of the pre-trained transformer blocks with transfer learning. However, the final classifier layer should be newly created. It is because the number of labels between MaskedLM and classification is different.</p>

<p>However, T5 suggested text-to-text in order to resolve the task with text generation by expressing the label in natural language. As such, T5 can utilize the same classifier layer that was used for pre-training. Multi-task learning and implementation to diverse tasks are possible utilizing this fact.</p>

<h2 id="korquad-10-the-korean-question-answering-dataset"><a href="https://korquad.github.io/KorQuad%201.0/">KorQuAD 1.0 (<strong>The Korean Question Answering Dataset</strong>)</a></h2>

<p>We will assess the performance of text-to-text with QA through korquad dataset. Of course, it was not as good as SOTA, we could compare the performance of pko-t5 with other T5.</p>

<p>Rather than finding span from BERT style context, T5 trains to directly create an answer when it comes to QA tasks as well.</p>

<table>
  <thead>
    <tr>
      <th>Model name</th>
      <th>Exact-match</th>
      <th>F1-score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>pko-t5-base</td>
      <td>83.32</td>
      <td>88.30</td>
    </tr>
    <tr>
      <td>pko-t5-large</td>
      <td><strong>86.28</strong></td>
      <td><strong>91.22</strong></td>
    </tr>
    <tr>
      <td>mt5-large</td>
      <td>82.05</td>
      <td>88.20</td>
    </tr>
  </tbody>
</table>

<p>The above is the outcome of T5’s training through korquad.</p>

<p>pko-t5 showed some effect with korquad as well. The performance between mt5-large and pko-t5-base was similar. In addition, pko-t5-large had better performance than mt5-large already.</p>

<p>We thought that pko-t5 was well-trained enough to go through KLUE.</p>

<h2 id="kluekorean-language-understanding-evaluation"><a href="https://klue-benchmark.com/">KLUE(<strong>Korean Language Understanding Evaluation</strong>)</a></h2>

<p>We evaluated with dev-set from KLUE as you can see from the table below. Baseline is the one with the best performance according to KLUE’s research paper.</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>Model</th>
      <th>tc (macro F1)</th>
      <th>sts (pearsonr/F1)</th>
      <th>nli (acc)</th>
      <th>ner (entity-level F1)</th>
      <th>re (micro F1)</th>
      <th>dp (LAS)</th>
      <th>mrc (EM/F1)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td> </td>
      <td>Baseline</td>
      <td><strong>87.30</strong></td>
      <td><strong>93.20/86.13</strong></td>
      <td><strong>89.50</strong></td>
      <td>86.06</td>
      <td>71.06</td>
      <td>87.93</td>
      <td>75.26/ -</td>
    </tr>
    <tr>
      <td>FT</td>
      <td>pko-t5-smal</td>
      <td>86.21</td>
      <td>77.99/77.01</td>
      <td>69.20</td>
      <td>82.60</td>
      <td>62.95</td>
      <td>93.15</td>
      <td>43.81/46.58</td>
    </tr>
    <tr>
      <td>FT</td>
      <td>pko-t5-base</td>
      <td>87.29</td>
      <td>90.25/83.43</td>
      <td>79.73</td>
      <td>87.80</td>
      <td>72.94</td>
      <td>97.28</td>
      <td>61.53/64.74</td>
    </tr>
    <tr>
      <td>FT</td>
      <td>pko-t5-large</td>
      <td>87.12</td>
      <td>92.05/85.24</td>
      <td>84.96</td>
      <td><strong>88.18</strong></td>
      <td>72.26</td>
      <td><strong>97.60</strong></td>
      <td>68.01/71.44</td>
    </tr>
    <tr>
      <td>MT</td>
      <td>pko-t5-small</td>
      <td>85.85</td>
      <td>79.12/77.81</td>
      <td>66.8</td>
      <td>81.53</td>
      <td>67.93</td>
      <td>91.38</td>
      <td>44.97/48.07</td>
    </tr>
    <tr>
      <td>MT</td>
      <td>pko-t5-base</td>
      <td>86.86</td>
      <td>87.61/81.42</td>
      <td>75.46</td>
      <td>86.85</td>
      <td>71.85</td>
      <td>96.32</td>
      <td>61.95/65.06</td>
    </tr>
    <tr>
      <td>MT</td>
      <td>pko-t5-large</td>
      <td>87.25</td>
      <td>91.05/84.58</td>
      <td>82.16</td>
      <td>87.63</td>
      <td><strong>74.78</strong></td>
      <td>97.33</td>
      <td>69.18/71.92</td>
    </tr>
  </tbody>
</table>

<p>In the table above, “FT” means single-task fine-tuning and MT means multitask fine-tuning. For “MT”, we changed all tasks into text-to-text format for training and analyzed the performance of each task.</p>

<p>The existing encoder-based models had better performance when it comes to TC, STS, NLI. I believe that T5 is insufficient because the training was text-to-text based.</p>

<p>However, we saw a much better performance when it comes to NER, RE, and DP compared to the existing encoder-based models. Our assumption is that the encoder-decoder model based on self-attention is more efficient than the encoder for the task.</p>

<p>We wanted MRC to include as much context as possible. Therefore, it included question, context and had the input of a maximum length of 1024. However, performance was significantly low for MRC. We needed a further experiment on this.</p>

<h3 id="evaluation-of-the-additional-experiment-on-klue-mrc">Evaluation of the additional experiment on Klue-MRC</h3>

<table>
  <thead>
    <tr>
      <th>모델명</th>
      <th>(1) EM / F1</th>
      <th>(2) EM / F1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>pko-t5-small</td>
      <td>42.20/45.03</td>
      <td>46.85/50.46</td>
    </tr>
    <tr>
      <td>pko-t5-base</td>
      <td>57.06/60.20</td>
      <td>63.12/67.38</td>
    </tr>
    <tr>
      <td>pko-t5-large</td>
      <td>61.53/64.94</td>
      <td>70.15/74.20</td>
    </tr>
  </tbody>
</table>

<ol>
  <li>Context sliding with max length set as 512 for training</li>
  <li>Included title in addition to context</li>
</ol>

<p>According to the experiment above, the performance gets better if title is included as well.</p>

<p>Currently, we started studying and developing for FiD since pko-t5 is now developed to a certain stage. For this purpose, we trained pko-t5 for Korean from scratch and had a good outcome. We would like to share the report on github and huggingface.</p>

<ul>
  <li>github: <a href="https://github.com/paust-team/pko-t5">https://github.com/paust-team/pko-t5</a></li>
  <li>huggingface: <a href="https://huggingface.co/paust/pko-t5-base">https://huggingface.co/paust/pko-t5-base</a></li>
</ul>

<p>That is how we developed pko-t5, the encoder-decoder model based on Korean. After its development, we have been utilizing it for diverse purposes.</p>

<p>Thank you for reading.</p>

<h1 id="acknowledgement">Acknowledgement</h1>

<p>This article was sponsored by TPU Research Cloud Program. Let us thanks provide for free TPUs by the Google TRC-Support Team.</p>


                <!-- Pagination links -->


            </article>

            

        </section>

        <!-- Add time bar only for pages without pagination -->
        

        <!-- Show modal if the post is the last one -->
        

        <!-- Show modal before user leaves the page -->
        

        <!-- Add your newsletter subscription form here -->

        <section class="share">
    <h3>Share</h3>
    <a aria-label="Share on Twitter" href="https://twitter.com/intent/tweet?text=&quot;PAUST shares the experiences for making pretrained model T5 by large-scale korean corpus&quot;%20/pko-t5-dev-diary/%20via%20&#64;&hashtags=modeling,shaple"
    onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;" title="Share on Twitter">
        <svg class="icon icon-twitter"><use xlink:href="#icon-twitter"></use></svg>
    </a>
    <a aria-label="Share on Facebook" href="https://www.facebook.com/sharer/sharer.php?u=/pko-t5-dev-diary/"
    onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;" title="Share on Facebook">
        <svg class="icon icon-facebook"><use xlink:href="#icon-facebook"></use></svg>
    </a>
</section>

        

  <section class="author">
    <div class="details">
      
        <img class="img-rounded" src="https://avatars.githubusercontent.com/u/44251210?s=400&u=b509971c0fe850f2154169f89f3eb2ffa059b918&v=4" alt="Dennis Park">
      
      <p class="def">Author</p>
      <h3 class="name">
        <a href="/authors/dennispark/">Dennis Park</a>
      </h3>
      <p class="desc">PAUST CTO</p>
      <p>
        
          <a href="https://github.com/1dennispark" title="Github">
            <svg><use xlink:href="#icon-github"></use></svg>
          </a>
        
        
        
        
        
        
      </p>
    </div>
  </section>

  
  
  
  
  
  
  

  <script type="application/ld+json">
  {
      "@context": "http://schema.org",
      "@type": "Person",
      "name": "Dennis Park",
      
      "image": "https://avatars.githubusercontent.com/u/44251210?s=400&u=b509971c0fe850f2154169f89f3eb2ffa059b918&v=4",
      
      "jobTitle": "Chief Technical Officer",
      "url": "/en/authors/dennispark/",
      "sameAs": [
        "https://github.com/1dennispark"
      ]
  }
  </script>


        

        <footer>
    <p>
      
        <a href="https://github.com/paust-team" title="Github">
          <svg><use xlink:href="#icon-github"></use></svg>
        </a>
      
      
      
      
      
      
    </p>

    <ul>
  
    
      <li>
        <a href="/en/">Home</a>
      </li>
    
  
</ul>


    <p>
      <span>Jekflix</span> was made with <svg class="love"><use xlink:href="#icon-heart"></use></svg> by <a href="https://rossener.com" target="_blank" class="creator">Thiago Rossener</a>
    </p>
</footer>









<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Organization",
  "name": "PAUST TechBlog",
  "description": "All of things about tech in PAUST:w",
  "url": "/en/",
  "logo": {
      "@type": "ImageObject",
      "url": "/en/assets/img/icons/mediumtile.png",
      "width": "600",
      "height": "315"
  },
  "sameAs": [
    "https://github.com/paust-team"
  ]
}
</script>

<!-- Include the script that allows Netlify CMS login -->
<script src="https://identity.netlify.com/v1/netlify-identity-widget.js"></script>

<!-- Include the website scripts -->
<script src="/en/assets/js/scripts.min.js"></script>

<!-- Include Google Analytics script -->
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-DMM1B77FTY"></script>
<script>
  var host = window.location.hostname;
  if (host != 'localhost') {
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-DMM1B77FTY');
  }
</script>
  


<!-- Include extra scripts -->



        

        
        
        
        
        
        
        
        
        <script type="application/ld+json">
        {
            "@context": "http://schema.org",
            "@type": "BlogPosting",
            "name": "pko-T5 Dev Case-study",
            "headline": "How PAUST developed T5 utilizing large-scale Korean corpus",
            "description": "PAUST shares the experiences for making pretrained model T5 by large-scale korean corpus",
            "image": "https://s.abcnews.com/images/Technology/AP_black_hole_kab_150226_16x9_992.jpg",
            "url": "/en/pko-t5-dev-diary/",
            "articleBody": "Introduction
PAUST has successfully developed Open-domain QA based on Korean with T5.

People have tried to utilize Facebook DPR and FiD to develop Open-domain QA.

There were Korean-based pre-training models for DPR, such as monologg/koBERT and klue/roberta-base. However, for FiD, we need an encoder-decoder model rather than an encoder-only model.

T5 and Bart are representing encoder-decoder models. We needed 2 conditions in the below when experimenting with utilizing the encoder-decoder model based on Korean.


  Make it available to receive special characters by making tokenizer based on BBPE.
  Make it based on T5 v1.1 model.


In order to satisfy the above conditions, we tried T5 model training utilizing the Korean text corpus and transformers library.

We easily made BBPE thanks to tokenizers library. For T5 training which was originally realized with tensorflow, we made one utilizing PyTorch model of transformers.

First of all, as you can see from Fig. 1, T5 integrates the format of all datasets into text-to-text format. Therefore, it is very useful for it is possible to implement it in diverse NLP tasks.


Fig.1 T5 integrating format into Text-to-text

This makes it available to be implemented for diverse NLP tasks such as Translation, Text classification, QA.

Creating a pre-training from scratch like this demands huge GPU resources. However, we needed to train diverse tasks with limited GPU resources. Therefore, it was very beneficial for us that it was okay to do some fine-tuning based on pre-trained knowledge for T5 model.

Please refer to the link for the differences between T5 v1.1 and original T5.

Implementation

Fortunately, we had useful Korean data such as “모두의말뭉치” by the National Institute of Korean Language, Namu Wiki, and Wikipedia. At least for T5 v1.1, it only needed unsupervised learning, which meant we could exclude multitask-learning in the original study which made things more convenient.

Although the beginning of actual training was from run_mlm_t5_flax.py which was provided as an example by huggingface/transformers, However, what Huggingface provides only decides the static ratio of T5 span to be created statically. Therefore, we couldn’t solely depend on Huggingface. So we did random active masking and then, merging of masked tokens. The code is below:
def _fill_in_the_blank(self, words: List[int]):
    &quot;&quot;&quot;
    input:
    {
      words: [My, name, is, T5, .]
    }
    output:
    {
      inputs: [&amp;lt;extra_id_1&amp;gt;, name, is, &amp;lt;extra_id_2&amp;gt;, .]
      outputs: [My, &amp;lt;extra_id_1&amp;gt;, T5, .]
    }
    &quot;&quot;&quot;
    mask_id = -1

    min_prob = 1 / (len(words) + 1)
    max_prob = 1 / 2
    inputs = copy.deepcopy(words)
    targets = words
    for i in range(len(words)):
        prob = random.random()
        if min_prob &amp;lt; prob &amp;lt; max_prob:
            inputs[i] = mask_id
        else:
            targets[i] = mask_id

    def merge_mask(words_):
        mask_spans = []
        begin, end = None, None
        for i, w in enumerate(words_):
            if w == mask_id:
                if begin is None:
                    begin = i
                end = i + 1
            else:
                if end is not None:
                    mask_spans.append((begin, end))
                    begin, end = None, None
        if begin is not None and end is not None:
            mask_spans.append((begin, end))

        new_words_ = []
        last_offset = 0
        assert len(mask_spans) &amp;lt;= len(self.extra_ids), f&quot;mask_spans={len(mask_spans)} is over length of extra_ids&quot;
        for i, (begin, end) in enumerate(mask_spans):
            new_words_ += words_[last_offset:begin]
            new_words_.append(self.extra_ids[i])
            last_offset = end
        new_words_ += words_[last_offset:]

        return new_words_

    inputs = merge_mask(inputs)
    targets = merge_mask(targets)

    return {'inputs': inputs, 'targets': targets}


We can prevent all targets to be masked from the code above if we set at least 1 token to be masked but limit the total percentage of tokens to be masked up to 50%.

T5 calls this task a “Span Corruption Task” and it works like the below:

SpanCorruptionTask(tokens=['My', 'name', 'is', 'T5', '.']) =&amp;gt; {
  inputs: ['&amp;lt;extra_id_1&amp;gt;', 'name', 'is', '&amp;lt;extra_id_2&amp;gt;', '.']
  outputs: ['My', '&amp;lt;extra_id_1&amp;gt;', 'T5', '.']
}


We utilized 8 pieces of A100 for pre-training, and it took 3 days, 16 days, and 26 days for small, base, and large respectively. We trained from 1 node of TPU v3-8 for fine-tuning of each task.

Detailed code for pre-training can be found here: https://github.com/paust-team/pko-t5

We named this T5 model to pko-t5, short for PAUST Korea T5. We did diverse experiments utilizing this training model later. Details can be found in the next section.

Experiments

NSMC

First, we transformed NSMC data into text-to-text format and experimented. Before processing the experiment, we utilized pko-t5 and gained the outcome below:


  
    
      Model name
      NSMC’s Accuracy
    
  
  
    
      KoBART-base*
      90.24
    
    
      KETI-AIR/ke-t5-small-ko
      89.37
    
    
      KETI-AIR/ke-t5-base-ko
      91.38
    
    
      KETI-AIR/ke-t5-large-ko
      90.67
    
    
      paust/pko-t5-small
      89.54
    
    
      paust/pko-t5-base
      91.05
    
    
      paust/pko-t5-large
      91.15
    
    
      google/mt5-base
      87.63
    
  


*: Please refer to KoBART-base announced by SKT.

Although we had less outcome than ke-t5, the performance was within the error range. We designed the document → label generation for training with NSMC data for the text-to-text performance.

For example, BERT style and text-to-text style proceeds the sentence below differently:

Input: &quot;pko-t5 프로젝트는 정말 멋지고 훌륭하네요.&quot;
BERT label: 1 (positive label)
T5 label: &quot;긍정적인 댓글&quot;


For encoder-only architecture like BERT, we can use the weights of the pre-trained transformer blocks with transfer learning. However, the final classifier layer should be newly created. It is because the number of labels between MaskedLM and classification is different.

However, T5 suggested text-to-text in order to resolve the task with text generation by expressing the label in natural language. As such, T5 can utilize the same classifier layer that was used for pre-training. Multi-task learning and implementation to diverse tasks are possible utilizing this fact.

KorQuAD 1.0 (The Korean Question Answering Dataset)

We will assess the performance of text-to-text with QA through korquad dataset. Of course, it was not as good as SOTA, we could compare the performance of pko-t5 with other T5.

Rather than finding span from BERT style context, T5 trains to directly create an answer when it comes to QA tasks as well.


  
    
      Model name
      Exact-match
      F1-score
    
  
  
    
      pko-t5-base
      83.32
      88.30
    
    
      pko-t5-large
      86.28
      91.22
    
    
      mt5-large
      82.05
      88.20
    
  


The above is the outcome of T5’s training through korquad.

pko-t5 showed some effect with korquad as well. The performance between mt5-large and pko-t5-base was similar. In addition, pko-t5-large had better performance than mt5-large already.

We thought that pko-t5 was well-trained enough to go through KLUE.

KLUE(Korean Language Understanding Evaluation)

We evaluated with dev-set from KLUE as you can see from the table below. Baseline is the one with the best performance according to KLUE’s research paper.


  
    
       
      Model
      tc (macro F1)
      sts (pearsonr/F1)
      nli (acc)
      ner (entity-level F1)
      re (micro F1)
      dp (LAS)
      mrc (EM/F1)
    
  
  
    
       
      Baseline
      87.30
      93.20/86.13
      89.50
      86.06
      71.06
      87.93
      75.26/ -
    
    
      FT
      pko-t5-smal
      86.21
      77.99/77.01
      69.20
      82.60
      62.95
      93.15
      43.81/46.58
    
    
      FT
      pko-t5-base
      87.29
      90.25/83.43
      79.73
      87.80
      72.94
      97.28
      61.53/64.74
    
    
      FT
      pko-t5-large
      87.12
      92.05/85.24
      84.96
      88.18
      72.26
      97.60
      68.01/71.44
    
    
      MT
      pko-t5-small
      85.85
      79.12/77.81
      66.8
      81.53
      67.93
      91.38
      44.97/48.07
    
    
      MT
      pko-t5-base
      86.86
      87.61/81.42
      75.46
      86.85
      71.85
      96.32
      61.95/65.06
    
    
      MT
      pko-t5-large
      87.25
      91.05/84.58
      82.16
      87.63
      74.78
      97.33
      69.18/71.92
    
  


In the table above, “FT” means single-task fine-tuning and MT means multitask fine-tuning. For “MT”, we changed all tasks into text-to-text format for training and analyzed the performance of each task.

The existing encoder-based models had better performance when it comes to TC, STS, NLI. I believe that T5 is insufficient because the training was text-to-text based.

However, we saw a much better performance when it comes to NER, RE, and DP compared to the existing encoder-based models. Our assumption is that the encoder-decoder model based on self-attention is more efficient than the encoder for the task.

We wanted MRC to include as much context as possible. Therefore, it included question, context and had the input of a maximum length of 1024. However, performance was significantly low for MRC. We needed a further experiment on this.

Evaluation of the additional experiment on Klue-MRC


  
    
      모델명
      (1) EM / F1
      (2) EM / F1
    
  
  
    
      pko-t5-small
      42.20/45.03
      46.85/50.46
    
    
      pko-t5-base
      57.06/60.20
      63.12/67.38
    
    
      pko-t5-large
      61.53/64.94
      70.15/74.20
    
  



  Context sliding with max length set as 512 for training
  Included title in addition to context


According to the experiment above, the performance gets better if title is included as well.

Currently, we started studying and developing for FiD since pko-t5 is now developed to a certain stage. For this purpose, we trained pko-t5 for Korean from scratch and had a good outcome. We would like to share the report on github and huggingface.


  github: https://github.com/paust-team/pko-t5
  huggingface: https://huggingface.co/paust/pko-t5-base


That is how we developed pko-t5, the encoder-decoder model based on Korean. After its development, we have been utilizing it for diverse purposes.

Thank you for reading.

Acknowledgement

This article was sponsored by TPU Research Cloud Program. Let us thanks provide for free TPUs by the Google TRC-Support Team.
",
            "wordcount": "1826",
            "inLanguage": "ko",
            "dateCreated": "2022-08-24/",
            "datePublished": "2022-08-24/",
            "dateModified": "2022-08-24/",
            "author": {
                "@type": "Person",
                "name": "Dennis Park",
                
                "image": "https://avatars.githubusercontent.com/u/44251210?s=400&u=b509971c0fe850f2154169f89f3eb2ffa059b918&v=4",
                
                "jobTitle": "Chief Technical Officer",
                "url": "/en/authors/dennispark/",
                "sameAs": [
                    "https://github.com/1dennispark"
                ]
            },
            "publisher": {
                "@type": "Organization",
                "name": "PAUST TechBlog",
                "url": "/en/",
                "logo": {
                    "@type": "ImageObject",
                    "url": "/en/assets/img/blog-image.png",
                    "width": "600",
                    "height": "315"
                }
            },
            "mainEntityOfPage": "True",
            "genre": "blog",
            "articleSection": "blog",
            "keywords": ["modeling","shaple"]
        }
        </script>
    </body>
</html>
